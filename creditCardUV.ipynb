{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "creditCardUV.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barrioBDes/creditCardFraud/blob/main/creditCardUV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7W9d_lee1hT"
      },
      "source": [
        "## Comprensión de negocio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZltKZPWHFll"
      },
      "source": [
        "La seguridad a la hora de realizar pagos con tarjeta de crédito es un asunto de gran interés tanto para entidades como para ladrones. Una entidad segura ahorrará dinero en indemnizaciones y ofrecerá un incentivo a quienes quieran tener seguros sus ahorros. Sobre este tema, hemos descargado un dataset de Kaggle conformado por 28 variables ofuscadas por razones de privacidad (o eso dicen), mediante Análisis de Componentes Principales.\n",
        "\n",
        "Hay dos variables que sí están disponibles, y son Time y Amount.\n",
        "\n",
        "De la variable Time no conocemos el punto de referencia (la transferencia 1), pero sí los minutos que han pasado desde ella. Para aprovechar el dato y convertirlo en algo útil para el modelo dividimos las operaciones en grupos de 3 horas, de forma que los horarios 'de noche' siempre sean de noche, y los 'de día', también estén marcados.\n",
        "\n",
        "La variable Amount también está marcada, entre operaciones de 'hasta 60 euros', entre 60 y 2000 y más de 2000. De este modo tenemos en cuenta operaciones contactless y operaciones muy altas (al menos, para mi criterio)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ll74UJe1hV"
      },
      "source": [
        "### Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEj7K2sPe1hX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "import lightgbm as lgb\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import keras\n",
        "from keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D, Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras import regularizers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from matplotlib import pyplot\n",
        "!pip install sklearn-contrib-py-earth\n",
        "from pyearth import Earth\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9WT1bIRcmFu"
      },
      "source": [
        "def matrizConfusion(cm):\n",
        "  print(cm)\n",
        "  sn.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', cmap='Blues')\n",
        "\n",
        "def AUC(fpr, tpr, threshold):\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "  plt.legend(loc = 'lower right')\n",
        "  plt.plot([0, 1], [0, 1],'r--')\n",
        "  plt.xlim([0, 1])\n",
        "  plt.ylim([0, 1])\n",
        "  plt.show()\n",
        "\n",
        "def comprobarNAs(data):\n",
        "  v = 0\n",
        "  for _ in data.columns:\n",
        "    if (data[_].isnull().sum() != 0):\n",
        "      print(_ + ' contiene NAs o NULL')\n",
        "      v = 1\n",
        "  if (v == 0):\n",
        "    print(\"No hay NAs en todo el dataset\")\n",
        "  return data\n",
        "\n",
        "def nuevaVariable1(data):\n",
        "  condiciones1 = [\n",
        "       (data['Time']/3600/2 < 3),\n",
        "       (data['Time']/3600/2 < 6),\n",
        "       (data['Time']/3600/2 < 9),\n",
        "       (data['Time']/3600/2 < 12),\n",
        "       (data['Time']/3600/2 < 15),\n",
        "       (data['Time']/3600/2 < 18),\n",
        "       (data['Time']/3600/2 < 21),\n",
        "       (data['Time']/3600/2 < 24)]\n",
        "  valor1 = ['A','B','C','D','E','F','G','H']\n",
        "  data['Hora'] = np.select(condiciones1, valor1, default='K')\n",
        "  data = pd.get_dummies(data, prefix=['Hora'])\n",
        "  data = data.drop(['Time','Hora_H'], axis=1)\n",
        "  return data\n",
        "\n",
        "def nuevaVariable2(data):\n",
        "  condiciones2 = [\n",
        "       (data['Amount'] < 60),\n",
        "       ((data['Amount'] >= 60) & (data['Amount'] <=2000)),\n",
        "       (data['Amount'] >2000)]\n",
        "  valor2 = ['A','B','C']\n",
        "  data['Cantidad'] = np.select(condiciones2, valor2, default='K')\n",
        "  data = pd.get_dummies(data, prefix=['Cantidad'])\n",
        "  data = data.drop(['Amount','Cantidad_C'], axis=1)\n",
        "  clase = data['Class']\n",
        "  data.drop(labels=['Class'], axis=1,inplace = True)\n",
        "  data.insert(len(list(data)), 'Class', clase)\n",
        "  return data\n",
        "\n",
        "def resampleyMinMax(data):\n",
        "  dataFraude = data.loc[data['Class'] == 1]\n",
        "  dataNoFraude = data.loc[data['Class'] == 0].sample(len(dataFraude)*4)\n",
        "  data = pd.concat([dataFraude, dataNoFraude])\n",
        "  scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "  dataEscalada = scaler.fit_transform(data)\n",
        "  nombresColumna = data.columns\n",
        "  data = pd.DataFrame(dataEscalada)\n",
        "  data.columns = nombresColumna\n",
        "  data = data.sample(frac=1).reset_index(drop=True)\n",
        "  X = np.array(data.loc[:, data.columns != 'Class'])\n",
        "  y = np.array(data.loc[:, data.columns == 'Class'])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=1) \n",
        "  sm = SMOTE(random_state=2)\n",
        "  X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
        "  print('Tras el resamplig, hay ' + str(sum(y_train_res==1)) + ' positivos y ' + str(sum(y_train_res==-1)) + ' negativos.')\n",
        "  return X_train, X_test, y_train, y_test, X_train, X_val, y_train, y_val, X_train_res, y_train_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwpEOEsZKVTZ"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/creditcard.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWx3JMtXKYn8"
      },
      "source": [
        "## Comprensión de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X74iA2OKcMr"
      },
      "source": [
        "Los datos, como hemos comentado, se componen de 28 columnas ofuscadas, una variable llamada Time y otra llamada Amount. Transformamos las dos últimas y comprobamos que no existen NAs en el resto. En las siguientes líneas hacemos lo que hemos comentado, y demás escalamos y normalizamos las variables (por si no lo estuvieran ya con el PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0vxIi2KX2Q"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSCiwnS6SlDY"
      },
      "source": [
        "data = comprobarNAs(data)\n",
        "data = nuevaVariable1(data)\n",
        "data = nuevaVariable2(data)\n",
        "X_train, X_test, y_train, y_test, X_train, X_val, y_train, y_val, X_train_res, y_train_res = resampleyMinMax(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X8QkoNgLnzO"
      },
      "source": [
        "## Modelos (1 - LightGBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_csQD_JLr_v"
      },
      "source": [
        "Cuando hacemos una predicción de clasificación con un modelo como LightGBM (o autoencoder, como veremos después), a nivel interno el algoritmo no decide \"1\" o \"0\", sino un scoring entre ambos que indica \"cómo de probable es que sea \"1\" o \"0\". Llevamos a cabo 100 rondas de LightGBM y tomamos únicamente la que mejor AUC parece devolver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM7INIgcLUl8"
      },
      "source": [
        "train_data = lgb.Dataset(X_train_res, label=y_train_res)\n",
        "test_data = lgb.Dataset(X_test, label=np.reshape(y_test, (1, len(y_test)))[0])\n",
        "\n",
        "parameters = {\n",
        "    'application': 'binary',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'is_unbalance': 'false',\n",
        "    'boosting': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.5,\n",
        "    'bagging_fraction': 0.5,\n",
        "    'bagging_freq': 20,\n",
        "    'learning_rate': 0.05,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "lightgbm = lgb.train(parameters,\n",
        "                       train_data,\n",
        "                       valid_sets=test_data,\n",
        "                       num_boost_round=100,\n",
        "                       early_stopping_rounds=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPoLmmnVe1i0"
      },
      "source": [
        "## Evaluación LightGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49nwNQQ7M37p"
      },
      "source": [
        "Como la categoría de clasificación es en realidad un número entre 0 y 1, dependiendo de dónde establezcamos el corte tendremos una precisión u otra. Esto es lo que evalúa la curva ROC, dibujando el porcentaje de aciertos que tendríamos para cada nivel de corte; calculando su área tenemos un indicador de precisión del modelo. Hemos dividido el dataset en Train, Test y Validación para todos estos cálculos, las evaluaciones se hacen siempre sobre Validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGNznxQIUnyk"
      },
      "source": [
        "ypred = lightgbm.predict(X_val, num_iteration=lightgbm.best_iteration)\n",
        "yval = np.reshape(y_val, (1, len(y_val)))\n",
        "fpr, tpr, threshold = metrics.roc_curve(np.resize(y_val, (1,len(y_val)))[0], ypred)\n",
        "AUC(fpr, tpr, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46TSGsyaNe1m"
      },
      "source": [
        "En la siguiente matriz de confusión observamos que devuelve 3 predicciones negativas que realmente fueran positivas (caso más desfavorable) y 0 'falsas alarmas' en las que predigamos un positivo que finalmente no lo es."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ-UCnaAduYf"
      },
      "source": [
        "valoresRealesValidacion = np.where(np.resize(y_val, (1,len(y_val)))[0]>0,1,0)\n",
        "valoresPredichosValidac = np.where(ypred>0.55,1,0)\n",
        "cm = confusion_matrix(valoresRealesValidacion,valoresPredichosValidac)\n",
        "matrizConfusion(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIJvBAlye1i2"
      },
      "source": [
        "## Modelo 2 - Red Neuronal Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_9y9UE_Pgak"
      },
      "source": [
        "Después de varias pruebas de configuración de capas y neuronas, el modelo parece converger en un 20% de precisión en 'test' en un modelo con claro sobreajuste (¿pocos datos?). \n",
        "\n",
        "Todo esto a juzgar por los gráficos devueltos por el History del modelo. No obstante podría tratarse de un error de escalas con la variable a predecir en Test, ya que finalmente su precisión es buena según la matriz de confusión y la curva ROC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZVFL6lTNidT"
      },
      "source": [
        "nb_epoch = 60 #60\n",
        "batch_size = 32 #16\n",
        "learning_rate = 0.05 #0.01\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "encoder = Dense(30, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
        "decoder = Dense(8, activation=\"relu\")(encoder)\n",
        "decoder = Dense(2, activation=\"sigmoid\")(encoder)\n",
        "decoder = Dense(8, activation=\"relu\")(decoder)\n",
        "decoder = Dense(30, activation=\"relu\")(decoder)\n",
        "decoder = Dense(1, activation=\"sigmoid\")(decoder)\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4XcYEZNRgBn"
      },
      "source": [
        "y_trainCero = np.where(np.resize(y_train, (1,len(y_train)))[0]>0,1,0)\n",
        "autoencoder.compile(metrics=['accuracy'],\n",
        "                    loss='mean_squared_error',\n",
        "                    optimizer='adam')\n",
        "\n",
        "cp = ModelCheckpoint(filepath=\"autoencoder_classifier.h5\",\n",
        "                               save_best_only=True,\n",
        "                               verbose=0)\n",
        "                               \n",
        "modelo = autoencoder.fit(X_train, y_trainCero,\n",
        "                    epochs=nb_epoch,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1,\n",
        "                    callbacks=[cp])\n",
        "                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU2XlWCFSmzO"
      },
      "source": [
        "print(modelo.history.keys())\n",
        "\n",
        "plt.plot(modelo.history['accuracy'])\n",
        "plt.plot(modelo.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(modelo.history['loss'])\n",
        "plt.plot(modelo.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQcZnhOFSu99"
      },
      "source": [
        "## Evaluación RN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwpQpP6DS0Bk"
      },
      "source": [
        "Se dan 9 casos en los que hay fraude pero indica que no, y 0 en los que avise de fraude y finalmente no lo haya. La el área bajo la curva ROC ronda el 0.96"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KihWfZ4ID6-8"
      },
      "source": [
        "valoresRealesValidacion = np.where(np.resize(y_val, (1,len(y_val)))[0]>0,1,0)\n",
        "valoresPredichosValidac = np.where(np.resize(modelo.model.predict(X_val), (1,len(modelo.model.predict(X_val))))>0.8,1,0)[0]\n",
        "cm = confusion_matrix(valoresRealesValidacion,valoresPredichosValidac)\n",
        "matrizConfusion(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm2EEMTZT4DI"
      },
      "source": [
        "fpr, tpr, threshold = metrics.roc_curve(valoresRealesValidacion, valoresPredichosValidac)\n",
        "AUC(fpr, tpr, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvx3j33me1i8"
      },
      "source": [
        "## Stacking - modelo Earth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKt7lBR2S3J7"
      },
      "source": [
        "Con las predicciones devueltas por el LightGBM y la red neuronal (la predicción real, no su traducción a 0 - 1, llevamos a cabo un Stacking con un modelo Earth.\n",
        "\n",
        "Para ello creamos un dataset con 197 registros, en el que añadimos el GBM predicho, el Autoencoder predicho, y el cuadrado y cubo de ambos, para que la nueva predicción pueda realizar splines. Además incluimos la variable real (1 o 0). Normalizamos y estandarizamos todo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxWnvD4YLe4q"
      },
      "source": [
        "prediccionGBMval = lightgbm.predict(X_val, num_iteration=lightgbm.best_iteration)\n",
        "prediccionENCval = np.resize(modelo.model.predict(X_val), (1,len(modelo.model.predict(X_val))))[0]\n",
        "yValidacion = np.where(np.resize(y_val, (1,len(y_val)))[0]>0,1,0)\n",
        "prediccionesVal = pd.DataFrame({\"GBMpredicho\": prediccionGBMval, \"ENCpredicho\": prediccionENCval, \"validacionReal\": yValidacion})\n",
        "prediccionesVal['GBMpredichoCuadrado'] = prediccionesVal['GBMpredicho']*prediccionesVal['GBMpredicho']\n",
        "prediccionesVal['GBMpredichoCubo'] = prediccionesVal['GBMpredicho']*prediccionesVal['GBMpredicho']*prediccionesVal['GBMpredicho']\n",
        "prediccionesVal['ENCpredichoCuadrado'] = prediccionesVal['ENCpredicho']*prediccionesVal['ENCpredicho']\n",
        "prediccionesVal['ENCpredichoCubo'] = prediccionesVal['ENCpredicho']*prediccionesVal['ENCpredicho']*prediccionesVal['ENCpredicho']\n",
        "prediccionesVal = prediccionesVal.reindex(columns=['GBMpredicho','GBMpredichoCuadrado','GBMpredichoCubo','ENCpredicho','ENCpredichoCuadrado','ENCpredichoCubo','validacionReal'])\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "prediccionesValEscalados = scaler.fit_transform(prediccionesVal)\n",
        "nombresColumna = prediccionesVal.columns\n",
        "prediccionesVal = pd.DataFrame(prediccionesValEscalados)\n",
        "prediccionesVal.columns = nombresColumna\n",
        "prediccionesVal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhkp_6_cO7kB"
      },
      "source": [
        "X = np.array(prediccionesVal.loc[:, prediccionesVal.columns != 'validacionReal'])\n",
        "y = np.array(prediccionesVal.loc[:, prediccionesVal.columns == 'validacionReal'])\n",
        "\n",
        "model = Earth()\n",
        "model.fit(X,y)\n",
        "    \n",
        "print(model.trace())\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1Ih1CITwEQ"
      },
      "source": [
        "## Evaluación stacking (Earth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWx12Lx_TzHD"
      },
      "source": [
        "La curva ROC tiene un área por debajo de 1.00 como resultado de haber combinado el modelo de LightGBM con la red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fydMOK_bR2Ko"
      },
      "source": [
        "EarthPrediccion = model.predict(X)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "EarthPrediccionRango = scaler.fit_transform(pd.DataFrame({\"EarthPrediccion\": EarthPrediccion}))\n",
        "EarthPrediccionRango = np.resize(EarthPrediccionRango, (1,len(EarthPrediccionRango)))\n",
        "valoresRealesValidacion = np.where(np.resize(y_val, (1,len(y_val)))[0]>0,1,0)\n",
        "valoresPredichosValidac = np.where(EarthPrediccionRango>0.75,1,0)[0]\n",
        "cm = confusion_matrix(valoresRealesValidacion,valoresPredichosValidac)\n",
        "matrizConfusion(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq_-_tqFSxsF"
      },
      "source": [
        "fpr, tpr, threshold = metrics.roc_curve(np.where(np.resize(y_val, (1,len(y_val)))[0]>0,1,0), EarthPrediccionRango[0])\n",
        "AUC(fpr, tpr, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8E9wdZFe1i9"
      },
      "source": [
        "## Evaluación stacking - GLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh1Ox-3tUMY4"
      },
      "source": [
        "Tal y como hemos hecho con el Earth, lanzamos un GLM. Este modelo es más explicativo que el anterior, y gracias a los p-valores podemos saber que la precisión del autoencoder es más fiable que la del GBM y se tiene más en cuenta para calcular el scoring final."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmfUgRjFNZ_H"
      },
      "source": [
        "!pip install PyGLM\n",
        "import statsmodels.api as sm\n",
        "import glm\n",
        "formula = 'validacionReal ~ GBMpredicho + GBMpredichoCuadrado + GBMpredichoCubo + ENCpredicho + ENCpredichoCuadrado + ENCpredichoCubo'\n",
        "glm = smf.glm(formula = formula, data=prediccionesVal, family=sm.families.Binomial())\n",
        "result = glm.fit()\n",
        "print(result.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRC8yGhxZKM4"
      },
      "source": [
        "X = pd.DataFrame(X)\n",
        "X.columns = nombresColumna[:-1]\n",
        "GLM = result.predict(X)\n",
        "fpr, tpr, threshold = metrics.roc_curve(np.where(np.resize(y_val, (1,len(y_val)))[0]>0,1,0), GLM)\n",
        "AUC(fpr, tpr, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyp5lA9IZUMl"
      },
      "source": [
        "cm = confusion_matrix(valoresRealesValidacion,np.where(np.resize(GLM.to_numpy(), (1,len(GLM.to_numpy())))[0]>0.1,1,0))\n",
        "matrizConfusion(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaidIbvQVF1g"
      },
      "source": [
        "## GLM sin variables ruidosas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVEB_UT8VLnQ"
      },
      "source": [
        "formula = 'validacionReal ~ ENCpredicho + ENCpredichoCuadrado + ENCpredichoCubo'\n",
        "glm = smf.glm(formula = formula, data=prediccionesVal, family=sm.families.Binomial())\n",
        "result = glm.fit()\n",
        "print(result.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWfipYHqU65q"
      },
      "source": [
        "X = pd.DataFrame(X)\n",
        "X.columns = nombresColumna[:-1]\n",
        "GLM = result.predict(X)\n",
        "fpr, tpr, threshold = metrics.roc_curve(np.where(np.resize(y_val, (1,len(y_val)))[0]>0,1,0), GLM)\n",
        "AUC(fpr, tpr, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vcDU_skVBl3"
      },
      "source": [
        "cm = confusion_matrix(valoresRealesValidacion,np.where(np.resize(GLM.to_numpy(), (1,len(GLM.to_numpy())))[0]>0.1,1,0))\n",
        "matrizConfusion(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}